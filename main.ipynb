{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b046907718094cceb02d95d40992c549",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Análisis de sentimiento en noticias financieras y conferencias de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "dcec312a750f458593be6741ae66d7b0",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Integrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0cc69a2fe12c4716bec891a2f5390f13",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Pablo González Barón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d1c348ca44c44f28ad329cc2ecbc8121",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "57d6261e11d44d3681e0f1e613c0bb3b",
    "deepnote_cell_type": "text-cell-number",
    "formattedRanges": [],
    "is_collapsed": false,
    "number": 2,
    "style": "decimal",
    "tags": []
   },
   "source": [
    "1. Clasificar las noticias financieras y las transcripciones de las conferencias de resultados de cada empresa como positivas, negativas o neutras para tomar decisiones de inversión sobre las acciones de cada empresa.\n",
    "2. Analizar una posible correlación entre los retornos diarios de las acciones de cada empresa junto con el sentimiento (positivo, negativo o neutro) de las noticias financieras y las transcripciones de conferencias de resultados de cada día.\n",
    "3. Caracterizar las noticias que causaron movimientos más agresivos en el precio de las acciones de ciertas empresas a las que estén relacionadas dichas noticias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c7471f32ecc640508e4fea10071a3e8d",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "da71c70ac4494a51a7d27767dcb0bcb9",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "Extraer una base de datos que contiene información de noticias y titulares financieros pertenecientes a las acciones de las empresas que componen el índice financiero S&P500 para luego posteriormente clasificar las noticias de cada empresa con el fin de tomar decisiones de inversión y mediante un análisis de sentimiento observar la relación que existe con los retornos diarios en la bolsa de valores de dichas empresas, para identificar como la polaridad del sentimiento está relacionada a cada ganancia o pérdida diaria del mercado. Adicional a esto, se extrae una base de datos por medio de web scraping de transcripciones de conferencias de resultados, donde se aplica análisis de sentimiento utilizando dos modelos distintos y viendo la relación que puede tener con los retornos diarios en la bolsa de valores de las respectivas empresas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cde5c9541058489696759c087a380f9b",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "### Datos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bc7c1b5c69dc4b899e8910f36d3fd761",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [
     {
      "fromCodePoint": 284,
      "ranges": [],
      "toCodePoint": 303,
      "type": "link",
      "url": "https://finnhub.io/"
     }
    ],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "En la actualidad existen muchas bases de datos en Kaggle con noticias y titulares financieros de varias empresas, pero estas bases de datos no están actualizadas y las noticias/titulares no corresponden a la actualidad, por tanto en nuestro caso utilizaremos una API llamada Finnhub (https://finnhub.io/) que nos permite traer noticias y titulares financieros de cualquier empresa y elegir las fechas de las cuales queremos noticias, esto es importante para poder comparar contra los retornos diarios de cada empresa en esas fechas elegidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "019889a43b6445fa9def554535408dc4",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "La versión gratuita de la API nos permite traer noticias de hasta un año atrás, esto es suficiente para poder realizar el análisis que necesitamos y entrenar el modelo a usar. La recolección de datos nos entrega la siguiente base de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las transcripciones de las conferencias de resultados, se aplicó web scraping en la página web https://capedge.com utilizando la librería Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "24c3d6110902420fa0d90d7df8c3f335",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1822,
    "execution_start": 1683524711956,
    "source_hash": "bfc0ddb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import pickle\n",
    "import swifter\n",
    "import warnings\n",
    "import itertools\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from cucco import Cucco\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar datos\n",
    "Las noticias financieras fueron extraídas en tres fechas distintas para agrupar la mayor cantidad de noticias en fechas distintas, por eso los datos vienen separados en 3 archivos distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6e0c64d003b14bc4ac72cbabb99c3f9b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 745,
    "execution_start": 1683524713749,
    "source_hash": "3332d9ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "d1 = pd.read_parquet('news_data.parquet.gzip')\n",
    "d2 = pd.read_parquet('news_data_2.parquet.gzip')\n",
    "d3 = pd.read_parquet('news_data_3.parquet.gzip')\n",
    "data = pd.concat([d1, d2, d3], ignore_index=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "8578dce19745485a9083f3e98b5624ca",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25,
    "execution_start": 1683524714477,
    "source_hash": "2007f58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c3b6ff559eb74a62bde6cc0f87ab7bea",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 38,
    "execution_start": 1683524714520,
    "source_hash": "878de0e4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "80466f5146604085b870a94fe6b0a6c2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 22,
    "execution_start": 1683524714565,
    "source_hash": "91e405f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[data.datetime < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fe57dcb5b9a94e62b52707509b5de127",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 121,
    "execution_start": 1683524714569,
    "source_hash": "150a4083",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removemos las fechas que son menores a 0, lo cual no tiene sentido.\n",
    "data = data[data.datetime > 0].copy()\n",
    "# Convertimos de formato unix a formato estándar.\n",
    "data['datetime'] = pd.to_datetime(data['datetime'],unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c660e87c48b540e38a9a05836e6c6f8f",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Análisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos un conteo de las noticias por categoria y por fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0dd45067658640f1a532b7f7c39f67df",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 392,
    "execution_start": 1683524714689,
    "source_hash": "48a46a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=data.category);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "fadbd2c4e5ee4739b0b8b8ced9c71ae2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 530,
    "execution_start": 1683524715081,
    "source_hash": "9b15a87",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.countplot(y=data.source);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0ae239d223c548aba26e48a4659189a4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 164,
    "execution_start": 1683524715602,
    "source_hash": "69751aeb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"headline\"].groupby([data[\"datetime\"].dt.year, data[\"datetime\"].dt.month]).count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9cf88b5857c94740ab660c0b4e549759",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Preprocesamiento del texto\n",
    "En primera instancia aplicamos normalización con ayuda de la librería Cucco, pues esta librería nos permite elegir un conjunto específico de normalizaciones. En este caso no aplicamos lematizacion, pues al hacer esto podemos afectar el contexto del corpus al aplicar el pipeline de spacytextblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b727f0eafb0d4703ba743a2f9380bcc6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10096,
    "execution_start": 1683524716194,
    "source_hash": "e2370526"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f3c32b53e8884d90963b31ed6a838a6e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1683524763456,
    "source_hash": "69f8729",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cucco = Cucco()\n",
    "\n",
    "normalizations = [\n",
    "    'remove_extra_white_spaces',\n",
    "    ('replace_punctuation', {'replacement': ''}),\n",
    "    ('replace_emojis', {'replacement': ''}),\n",
    "    ('replace_symbols', {'replacement': ''}),\n",
    "    ('replace_urls', {'replacement': ''}),\n",
    "]\n",
    "\n",
    "data['headline_clean'] = data['headline'].swifter.apply(lambda x : cucco.normalize(x, normalizations))\n",
    "data['summary_clean'] = data['summary'].swifter.apply(lambda x : cucco.normalize(x, normalizations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6cddf57d556c4a02a7b63514b306ecb4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### spacytextblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "86a9b700045f48fe8509464913b2bcf3",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1886,
    "execution_start": 1683524772012,
    "source_hash": "1d2e4672"
   },
   "outputs": [],
   "source": [
    "# Aplicamos SpaCy NLP\n",
    "data['headline_doc'] = data['headline_clean'].swifter.apply(lambda x : nlp(x))\n",
    "data['summary_doc'] = data['summary_clean'].swifter.apply(lambda x : nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "071520af09814dc7a0188d87204fb07b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "19266fe"
   },
   "outputs": [],
   "source": [
    "# Extraemos la polaridad de spacytextblob\n",
    "data['headline_polarity'] = data['headline_doc'].swifter.apply(lambda x : x._.blob.polarity)\n",
    "data['summary_polarity'] = data['summary_doc'].swifter.apply(lambda x : x._.blob.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ae28271792514c388cf9c027757cf4be",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "4934382a"
   },
   "outputs": [],
   "source": [
    "# Graficamos la distribución de la polaridad de los titulares y los resumenes de las noticias.\n",
    "plt.hist(data.groupby(['related'])['headline_polarity'].mean(), alpha=.8, label='headline polarity')\n",
    "plt.hist(data.groupby(['related'])['summary_polarity'].mean(), alpha=.8, label='summary polarity')\n",
    "plt.grid()\n",
    "plt.title('Histogram of summary column polarity scores across tickers')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos el modelo FinBERT pre-entrenado de huggingface.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el pipeline de analisis de sentimiento.\n",
    "sentimentanalysis_nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos el pipeline de analisis de sentimiento de forma paralelizada (usando todos los hilos disponibles del procesador) usando swifter.\n",
    "df['headline_sentiment'] = df['headline'].swifter.apply(lambda x : sentimentanalysis_nlp(x)[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los resultados parciales hasta ahora en un archivo pickle.\n",
    "data.to_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de modelos y caracterización de noticias influyentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargamos el archivo pickle guardado anteriormente.\n",
    "df = pd.read_pickle('data.pkl')\n",
    "# Para remover con el fin de caracterizar las noticias mas adelante\n",
    "removal = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extraemos datos de los precios para todos los tickers usando yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "master_prices = {}\n",
    "tickers = df.related.unique()\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(ticker)\n",
    "    t = yf.Ticker(ticker)\n",
    "    \n",
    "    # Extraemos unicamente los precios en las fechas para las cuales hay noticias.\n",
    "    related_df = df[df.related == ticker].copy()\n",
    "    price_history = t.history(start=related_df.datetime.min().strftime('%Y-%m-%d'), \n",
    "                              end=related_df.datetime.max().strftime('%Y-%m-%d'))[['Close']].reset_index()\n",
    "    price_history['Ticker'] = ticker\n",
    "    price_history.columns = ['datetime', 'close', 'ticker']\n",
    "    master_prices[ticker] = price_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos las correlaciones de la polaridad capturada junto con el retorno, para cada ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_corrs = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    returns = master_prices[ticker].set_index('datetime')[['close']].pct_change().iloc[1:].reset_index()\n",
    "    news = df[df.related == ticker]\n",
    "    \n",
    "    # Formateamos la columna 'datetime' ya que por esa columna haremos un inner join.\n",
    "    news['datetime'] = news['datetime'].apply(lambda x : pd.to_datetime(x.strftime('%Y-%m-%d')))\n",
    "    returns['datetime'] = returns['datetime'].apply(lambda x : pd.to_datetime(x.strftime('%Y-%m-%d')))\n",
    "    polarities = news.groupby(['datetime'])[['headline_polarity']].mean().reset_index()\n",
    "    res = pd.merge(polarities, returns, on='datetime', how='inner')\n",
    "    \n",
    "    # La correlación es calculada con el retorno del dia siguiente, asumiendo que el impacto de la noticia es un dia despues.\n",
    "    res['headline_polarity'] = res['headline_polarity'].shift(1)\n",
    "    res = res.iloc[1:]\n",
    "    res = res.set_index('datetime').corr().iloc[0].iloc[1]\n",
    "    master_corrs[ticker] = res\n",
    "    \n",
    "corrs = pd.DataFrame([master_corrs]).T.dropna()\n",
    "corrs.columns = ['corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "font = {'size'   : 22}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Graficamos la distribución de las correlaciones\n",
    "corrs.plot(kind='hist', color='#35B276')\n",
    "plt.grid();\n",
    "plt.savefig('spacytextblob_corr.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 tickers con mas correlación\n",
    "corrs.sort_values(by='corr', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 tickers con menos correlación\n",
    "corrs.sort_values(by='corr', ascending=False).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlaciones FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Función auxiliar para categorizar el sentimiento de FinBERT en 0, 1 o -1.\n",
    "def categorize_sentiment(x):\n",
    "    if x == 'neutral':\n",
    "        return 0\n",
    "    elif x == 'positive':\n",
    "        return 1\n",
    "    elif x == 'negative':\n",
    "        return -1\n",
    "    \n",
    "# Función auxiliar para categorizar los valores flotantes de los retornos.\n",
    "def categorize_returns(x, mean):\n",
    "    if abs(x) < 1.5*abs(mean):\n",
    "        return 0\n",
    "    elif x > 0:\n",
    "        return 1\n",
    "    elif x < 0:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos las correlaciones del sentimiento capturado junto con el retorno, para cada ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_corrs_finbert = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    returns = master_prices[ticker].set_index('datetime')[['close']].pct_change().iloc[1:].reset_index()\n",
    "    news = df[df.related == ticker]\n",
    "    news['datetime'] = news['datetime'].apply(lambda x : pd.to_datetime(x.strftime('%Y-%m-%d')))\n",
    "    returns['datetime'] = returns['datetime'].apply(lambda x : pd.to_datetime(x.strftime('%Y-%m-%d')))\n",
    "    \n",
    "    # Aplicamos las funciones auxiliares\n",
    "    news['headline_sentiment_num'] = news['headline_sentiment'].apply(lambda x : categorize_sentiment(x))\n",
    "    returns['close_num'] = returns['close'].apply(lambda x : categorize_returns(x, returns.close.mean()))\n",
    "    sentiments = news.groupby(['datetime'])[['headline_sentiment_num']].agg(lambda x: 1 if pd.Series.mean(x) > 0 else -1).reset_index()\n",
    "    res = pd.merge(sentiments, returns, on='datetime', how='inner')[['datetime', 'headline_sentiment_num', 'close_num']]\n",
    "    \n",
    "    # La correlación es calculada con el retorno del dia siguiente, asumiendo que el impacto de la noticia es un dia despues.\n",
    "    res['headline_sentiment_num'] = res['headline_sentiment_num'].shift(1)\n",
    "    res = res.iloc[1:]\n",
    "    res = res.set_index('datetime').corr().iloc[0].iloc[1]\n",
    "    master_corrs_finbert[ticker] = res\n",
    "    \n",
    "corrs_finbert = pd.DataFrame([master_corrs_finbert]).T.dropna()\n",
    "corrs_finbert.columns = ['corr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "font = {'size'   : 22}\n",
    "import matplotlib\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Graficamos la distribución de las correlaciones\n",
    "corrs_finbert.plot(kind='hist', color='#35B276')\n",
    "plt.grid();\n",
    "plt.savefig('finbert_corr.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 tickers con mas correlación\n",
    "corrs_finbert.sort_values(by='corr', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5 tickers con menos correlación\n",
    "corrs_finbert.sort_values(by='corr', ascending=False).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracterizar noticias que más movieron el mercado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_movernews = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    returns = master_prices[ticker].set_index('datetime')[['close']].pct_change().iloc[1:]\n",
    "    \n",
    "    # Extraemos los retornos atípicos\n",
    "    lowerq = returns.quantile(.05).iloc[0]\n",
    "    upperq = returns.quantile(.95).iloc[0]\n",
    "    outliers = returns[(returns.close < lowerq) | (returns.close > upperq)].reset_index()\n",
    "    \n",
    "    # Formatemos la columna 'datetime'\n",
    "    outliers['datetime'] = outliers['datetime'].apply(lambda x : pd.to_datetime(x.strftime('%Y-%m-%d')))\n",
    "    ticker_news = df[df.related == ticker].copy()\n",
    "    ticker_news['datetime'] = ticker_news['datetime'].apply(lambda x : pd.to_datetime(x.strftime('%Y-%m-%d')))\n",
    "    \n",
    "    news_outliers = {}\n",
    "    for index, row in outliers.iterrows():\n",
    "        \n",
    "        # Utilizamos la fecha menos un día, asumiendo que el impacto de la noticia fue un día después.\n",
    "        datestr = (row['datetime'] - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        res = ticker_news[ticker_news.datetime == datestr]['index'].tolist()\n",
    "        \n",
    "        if len(res) > 0:\n",
    "            res = ', '.join([str(a) for a in res])\n",
    "        else:\n",
    "            res = '-'\n",
    "            \n",
    "        if row['close'] > 0:\n",
    "            sentiment = 'positive'\n",
    "        elif row['close'] < 0:\n",
    "            sentiment = 'negative'\n",
    "            \n",
    "        news_outliers[datestr] = (res, sentiment)\n",
    "      \n",
    "    mover_news = pd.DataFrame(news_outliers).T.reset_index()\n",
    "    if len(mover_news) > 0:\n",
    "        mover_news.columns = ['datetime', 'id_news', 'sentiment']\n",
    "        mover_news['ticker'] = ticker\n",
    "        master_movernews.append(mover_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_movernews = pd.concat(master_movernews)\n",
    "master_movernews['n_ofnews'] = master_movernews['id_news'].swifter.apply(lambda x : len(x.split(', ')) if x != '-' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extraer IDs de las noticias positivas y negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negative_news = master_movernews[master_movernews.sentiment == 'negative'].copy()\n",
    "negative_ids = negative_news.id_news.tolist()\n",
    "negative_ids = [elem.split(', ') for elem in negative_ids if elem != '-']\n",
    "\n",
    "# Convertimos una lista de listas en una sola lista con la libreria itertools.\n",
    "negative_ids = list(itertools.chain.from_iterable(negative_ids))\n",
    "negative_ids = [int(i) for i in negative_ids]\n",
    "\n",
    "positive_news = master_movernews[master_movernews.sentiment == 'positive'].copy()\n",
    "positive_ids = positive_news.id_news.tolist()\n",
    "positive_ids = [elem.split(', ') for elem in positive_ids if elem != '-']\n",
    "\n",
    "# Convertimos una lista de listas en una sola lista con la libreria itertools.\n",
    "positive_ids = list(itertools.chain.from_iterable(positive_ids))\n",
    "positive_ids = [int(i) for i in positive_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extraemos los textos crudos de las noticias positivas y negativas, teniendo en cuenta los IDs que extraimos anteriormente.\n",
    "negative_texts = df[df['index'].isin(negative_ids)].copy()\n",
    "positive_texts = df[df['index'].isin(positive_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negative_texts.shape, positive_texts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizamos el texto del headline y eliminamos stopwords para aplicar la caracterización (nube de palabras y TM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negative_texts['headline_tm'] = negative_texts['headline_doc'].swifter.apply(lambda x : [token.lemma_.lower() for token in x if token.pos_ not in removal and not token.is_stop and token.is_alpha])\n",
    "positive_texts['headline_tm'] = positive_texts['headline_doc'].swifter.apply(lambda x : [token.lemma_.lower() for token in x if token.pos_ not in removal and not token.is_stop and token.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_texts.shape, negative_texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convertimos una lista de listas en una sola lista con la libreria itertools.\n",
    "negwords = list(itertools.chain.from_iterable(negative_texts.headline_tm.tolist()))\n",
    "poswords = list(itertools.chain.from_iterable(positive_texts.headline_tm.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(negwords), len(poswords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos las nubes de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = STOPWORDS\n",
    "# Aceptamos como máximo 200 palabras y aplicamos la nube de palabras sobre el corpus completo de palabras negativas o positivas.\n",
    "wordcloud_neg = WordCloud(width=1920, height=1080, stopwords=stopwords, background_color=\"white\", max_words=200).generate(' '.join(negwords))\n",
    "wordcloud_pos = WordCloud(width=1920, height=1080, stopwords=stopwords, background_color=\"white\", max_words=200).generate(' '.join(poswords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (26, 6)\n",
    "\n",
    "# Graficamos la nube de palabras de noticias negativas\n",
    "plt.imshow(wordcloud_neg)\n",
    "plt.savefig('negative_wordcloud.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Graficamos la nube de palabras de noticias positivas\n",
    "plt.imshow(wordcloud_pos)\n",
    "plt.savefig('positive_wordcloud.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling (no se usó)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# content = negative_texts['headline_tm'].tolist()\n",
    "# id2word = Dictionary(content)\n",
    "# id2word.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "# corpus = [id2word.doc2bow(text) for text in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topics = []\n",
    "# score = []\n",
    "# for i in range(1,10,1):\n",
    "#     lda_model = LdaMulticore(corpus=corpus, id2word=id2word, iterations=10, num_topics=i, workers = 12, passes=10, random_state=100)\n",
    "#     cm = CoherenceModel(model=lda_model, texts = content, corpus=corpus, dictionary=id2word, coherence='c_v')\n",
    "#     topics.append(i)\n",
    "#     score.append(cm.get_coherence())\n",
    "# _=plt.plot(topics, score)\n",
    "# _=plt.xlabel('Number of Topics')\n",
    "# _=plt.ylabel('Coherence Score')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lda_model = LdaMulticore(corpus=corpus, id2word=id2word, iterations=10, num_topics=3, workers=12, passes=10, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# p = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# content = positive_texts['headline_tm'].tolist()\n",
    "# id2word = Dictionary(content)\n",
    "# id2word.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "# corpus = [id2word.doc2bow(text) for text in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lda_model = LdaMulticore(corpus=corpus, id2word=id2word, iterations=10, num_topics=3, workers=12, passes=10, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# p = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "# p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c99f8b9f-730f-4242-befc-1feb4682d851' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "af6992585ac34346aac64291a0779133",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
